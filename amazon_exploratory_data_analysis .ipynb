{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting and working with data\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviews df\n",
    "with open('/Users/jbpatty/project-4/reviews_compiled.pkl','rb') as read_file:\n",
    "    reviews_compiled = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(428177, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_compiled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    0.914535\n",
       "negative    0.085465\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that the dataset has mostly positive reviews\n",
    "reviews_compiled.sentiment.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_text</th>\n",
       "      <th>asin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>353261</th>\n",
       "      <td>positive</td>\n",
       "      <td>you almost cant find these bad boys anywhere a...</td>\n",
       "      <td>B0016MHL6W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256172</th>\n",
       "      <td>positive</td>\n",
       "      <td>I make tea with leave and I ground some down a...</td>\n",
       "      <td>B000UY86DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566119</th>\n",
       "      <td>positive</td>\n",
       "      <td>My mom loves it. It's perfect for breakfast  l...</td>\n",
       "      <td>B002SWB73C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442099</th>\n",
       "      <td>positive</td>\n",
       "      <td>great product  fast delivery!</td>\n",
       "      <td>B001ACDOA0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328991</th>\n",
       "      <td>positive</td>\n",
       "      <td>Great deal as an add-on.</td>\n",
       "      <td>B0012OTF3Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment                                        review_text  \\\n",
       "353261  positive  you almost cant find these bad boys anywhere a...   \n",
       "256172  positive  I make tea with leave and I ground some down a...   \n",
       "566119  positive  My mom loves it. It's perfect for breakfast  l...   \n",
       "442099  positive                      great product  fast delivery!   \n",
       "328991  positive                           Great deal as an add-on.   \n",
       "\n",
       "              asin  \n",
       "353261  B0016MHL6W  \n",
       "256172  B000UY86DE  \n",
       "566119  B002SWB73C  \n",
       "442099  B001ACDOA0  \n",
       "328991  B0012OTF3Q  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_compiled.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Process the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviews - text preprocessing steps - remove numbers, captial letters, punctuation, and repetitive letters\n",
    "def preprocess(docs):\n",
    "    \"\"\"\n",
    "    Preprocess a corpus (Series) of documents before using a vectorizer\n",
    "    - remove numbers and punctuation\n",
    "    - convert all text to lower case\n",
    "    \"\"\"\n",
    "    # Remove numbers and punctuation\n",
    "    alphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n",
    "    \n",
    "    # Convert all text to lowercase\n",
    "    punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n",
    "    \n",
    "    # Remove repetitve letters \n",
    "    repeat_pattern = lambda x: re.sub(r'(\\w)\\1*', r'\\1', x)\n",
    "    \n",
    "    return docs.map(alphanumeric).map(punc_lower).map(repeat_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_compiled['review_text'] = preprocess(reviews_compiled['review_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to access later\n",
    "reviews_compiled.to_pickle('/Users/jbpatty/project-4/reviews_compiled_EDA.to_pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/jbpatty/project-4/reviews_compiled_EDA.to_pickle','rb') as read_file:\n",
    "    reviews_compiled = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X and y data sets\n",
    "X = reviews_compiled.review_text\n",
    "y = reviews_compiled.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first document-term matrix has default Count Vectorizer values - counts of unigrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv1 = CountVectorizer(stop_words='english')\n",
    "\n",
    "X_train_cv1 = cv1.fit_transform(X_train)\n",
    "X_test_cv1  = cv1.transform(X_test)\n",
    "\n",
    "pd.DataFrame(X_train_cv1.toarray(), columns=cv1.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second document-term matrix has both unigrams and bigrams, and indicators instead of counts\n",
    "cv2 = CountVectorizer(ngram_range=(1,2), binary=True, stop_words='english')\n",
    "\n",
    "X_train_cv2 = cv2.fit_transform(X_train)\n",
    "X_test_cv2  = cv2.transform(X_test)\n",
    "\n",
    "pd.DataFrame(X_train_cv2.toarray(), columns=cv2.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a logistic regression model to use\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the first model\n",
    "lr.fit(X_train_cv1, y_train)\n",
    "y_pred_cv1 = lr.predict(X_test_cv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the second model\n",
    "lr.fit(X_train_cv2, y_train)\n",
    "y_pred_cv2 = lr.predict(X_test_cv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to calculate the error metrics, since we'll be doing this several times\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "def conf_matrix(actual, predicted):\n",
    "    cm = confusion_matrix(actual, predicted)\n",
    "    sns.heatmap(cm, xticklabels=['predicted_negative', 'predicted_positive'], \n",
    "                yticklabels=['actual_negative', 'actual_positive'], annot=True,\n",
    "                fmt='d', annot_kws={'fontsize':20}, cmap=\"YlGnBu\");\n",
    "\n",
    "    true_neg, false_pos = cm[0]\n",
    "    false_neg, true_pos = cm[1]\n",
    "\n",
    "    accuracy = round((true_pos + true_neg) / (true_pos + true_neg + false_pos + false_neg),3)\n",
    "    precision = round((true_pos) / (true_pos + false_pos),3)\n",
    "    recall = round((true_pos) / (true_pos + false_neg),3)\n",
    "    f1 = round(2 * (precision * recall) / (precision + recall),3)\n",
    "\n",
    "    cm_results = [accuracy, precision, recall, f1]\n",
    "    return cm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The heat map for the first logistic regression model\n",
    "cm1 = conf_matrix(y_test, y_pred_cv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The heat map for the second logistic regression model\n",
    "cm2 = conf_matrix(y_test, y_pred_cv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all of the error metrics into a dataframe for comparison\n",
    "results = pd.DataFrame(list(zip(cm1, cm2)))\n",
    "results = results.set_index([['Accuracy', 'Precision', 'Recall', 'F1 Score']])\n",
    "results.columns = ['LogReg1', 'LogReg2']\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the two models, the second model outperforms the first model in all performance categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the first Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_cv1, y_train)\n",
    "\n",
    "y_pred_cv1_nb = mnb.predict(X_test_cv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the second Naive Bayes model\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train_cv2, y_train)\n",
    "\n",
    "y_pred_cv2_nb = bnb.predict(X_test_cv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the heat map for the first Naive Bayes model\n",
    "cm3 = conf_matrix(y_test, y_pred_cv1_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the heat map for the second Naive Bayes model\n",
    "cm4 = conf_matrix(y_test, y_pred_cv2_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all of the error metrics into a dataframe for comparison\n",
    "results_nb = pd.DataFrame(list(zip(cm3, cm4)))\n",
    "results_nb = results_nb.set_index([['Accuracy', 'Precision', 'Recall', 'F1 Score']])\n",
    "results_nb.columns = ['NB1', 'NB2']\n",
    "results_nb\n",
    "\n",
    "results = pd.concat([results, results_nb], axis=1)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second Logistic Regression model outperforms both Naive Bayes models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to access later\n",
    "results.to_pickle('/Users/jbpatty/project-4/results.to_pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/jbpatty/project-4/results.to_pickle','rb') as read_file:\n",
    "    reviews_compiled = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TF-IDF instead of Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF versions of the Count Vectorizers created earlier in the exercise\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf1 = TfidfVectorizer(stop_words='english')\n",
    "X_train_tfidf1 = tfidf1.fit_transform(X_train)\n",
    "X_test_tfidf1  = tfidf1.transform(X_test)\n",
    "\n",
    "tfidf2 = TfidfVectorizer(ngram_range=(1,2), binary=True, stop_words='english')\n",
    "X_train_tfidf2 = tfidf2.fit_transform(X_train)\n",
    "X_test_tfidf2  = tfidf2.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the first logistic regression on the TF-IDF data\n",
    "lr.fit(X_train_tfidf1, y_train)\n",
    "y_pred_tfidf1_lr = lr.predict(X_test_tfidf1)\n",
    "cm5 = conf_matrix(y_test, y_pred_tfidf1_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the second logistic regression on the TF-IDF data\n",
    "lr.fit(X_train_tfidf2, y_train)\n",
    "y_pred_tfidf2_lr = lr.predict(X_test_tfidf2)\n",
    "cm6 = conf_matrix(y_test, y_pred_tfidf2_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the first Naive Bayes model on the TF-IDF data\n",
    "mnb.fit(X_train_tfidf1.toarray(), y_train)\n",
    "y_pred_tfidf1_nb = mnb.predict(X_test_tfidf1)\n",
    "cm7 = conf_matrix(y_test, y_pred_tfidf1_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the second Naive Bayes model on the TF-IDF data\n",
    "bnb.fit(X_train_tfidf2.toarray(), y_train)\n",
    "y_pred_tfidf2_nb = bnb.predict(X_test_tfidf2)\n",
    "cm8 = conf_matrix(y_test, y_pred_tfidf2_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all of the error metrics into a dataframe for comparison\n",
    "results_tf = pd.DataFrame(list(zip(cm5, cm6, cm7, cm8)))\n",
    "results_tf = results_tf.set_index([['Accuracy', 'Precision', 'Recall', 'F1 Score']])\n",
    "results_tf.columns = ['LR1-TFIDF', 'LR2-TFIDF', 'NB1-TFIDF', 'NB2-TFIDF']\n",
    "results_tf\n",
    "\n",
    "results = pd.concat([results, results_tf], axis=1)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using TF-IDF, I was able to improve the recall, but the accuracy and precision of the first Naive Bayes model, but not enought to outperforms the second Logistic Model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
